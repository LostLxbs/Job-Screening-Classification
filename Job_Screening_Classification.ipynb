{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb86003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# All Imports\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import shutil\n",
    "import threading\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_curve, roc_auc_score, log_loss, classification_report\n",
    ")\n",
    "\n",
    "# Plotting Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# GUI Imports\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from tkinterdnd2 import DND_FILES, TkinterDnD\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfe99c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Model Training :\n",
      "Successfully loaded Resume_Dataset.csv. Shape: (10000, 15)\n",
      "Positive class 'Shortlisted' is encoded as: 1\n",
      "Total features after processing: 16\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Grid Search Complete :\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Model Trained Successfully! All components are in global memory.\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "# Global variables that all other blocks will use\n",
    "best_model = None\n",
    "preprocessor = None\n",
    "le_target = None\n",
    "feature_columns = []\n",
    "\n",
    "# Define Features, Target, and Categorical Columns\n",
    "feature_columns = [\n",
    "    'Programming_Skills',\n",
    "    'Coding_Skills_Years',\n",
    "    'Job_Experience_NumCompanies',\n",
    "    'Previous_Salary',\n",
    "    'Desired_Salary',\n",
    "    'Projects',\n",
    "    'Internships',\n",
    "    'Education',\n",
    "    'Gap_Years'\n",
    "]\n",
    "target_column = 'Shortlisting_class'\n",
    "\n",
    "# Separate features into numerical and categorical\n",
    "numerical_features = [\n",
    "    'Programming_Skills',\n",
    "    'Coding_Skills_Years',\n",
    "    'Job_Experience_NumCompanies',\n",
    "    'Previous_Salary',\n",
    "    'Desired_Salary',\n",
    "    'Projects',\n",
    "    'Internships',\n",
    "    'Gap_Years'\n",
    "]\n",
    "categorical_features = [\"Education\"]\n",
    "\n",
    "positive_label_encoded = 1\n",
    "\n",
    "X_test_processed = None\n",
    "y_test = None\n",
    "\n",
    "print(\"Starting Model Training :\")\n",
    "try:\n",
    "    df = pd.read_csv(\"Resume_Dataset.csv\")\n",
    "    print(f\"Successfully loaded Resume_Dataset.csv. Shape: {df.shape}\")\n",
    "\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Label Encoding\n",
    "    le_target = LabelEncoder()\n",
    "    y_encoded = le_target.fit_transform(y)\n",
    "    \n",
    "    if 'Shortlisted' in le_target.classes_:\n",
    "        positive_label_encoded = le_target.transform(\n",
    "            [c for c in le_target.classes_ if 'shortlist' in c.lower()]\n",
    "        )[0]\n",
    "        print(f\"Positive class 'Shortlisted' is encoded as: {positive_label_encoded}\")\n",
    "\n",
    "    # Preprocessing Pipeline\n",
    "    # Create the transformer for numerical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "\n",
    "    # Create the transformer for categorical features\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine transformers using ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Data Split (BEFORE processing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    # Apply preprocessing\n",
    "    # Fit the preprocessor on the training data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    # Only transform the test data\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Get feature names after OHE for better model understanding\n",
    "    ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "    all_feature_names = numerical_features + list(ohe_feature_names)\n",
    "    print(f\"Total features after processing: {len(all_feature_names)}\")\n",
    "\n",
    "    # 7. Model Training (Grid Search)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        param_grid, \n",
    "        cv=3, \n",
    "        n_jobs=-1, \n",
    "        scoring='f1_macro',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train_processed, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    print(\"\\nGrid Search Complete :\")\n",
    "    print(f\"Best Parameters: {grid.best_params_}\")\n",
    "    print(\"Model Trained Successfully! All components are in global memory.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Resume_Dataset.csv' not found.\")\n",
    "    print(\"Please make sure the dataset is in the same folder as this notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"Model training failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4954ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Feature Importances(Highest to lowest):\n",
      "                           Feature  Importance\n",
      "5                         Projects    0.268550\n",
      "1              Coding_Skills_Years    0.267273\n",
      "3                  Previous_Salary    0.182025\n",
      "4                   Desired_Salary    0.091740\n",
      "6                      Internships    0.068024\n",
      "0               Programming_Skills    0.061563\n",
      "2      Job_Experience_NumCompanies    0.042426\n",
      "7                        Gap_Years    0.011910\n",
      "8                   Education_B.E.    0.001982\n",
      "12                Education_M.Tech    0.001294\n",
      "14                   Education_MCA    0.000998\n",
      "10  Education_BSc Computer Science    0.000797\n",
      "15                   Education_MSc    0.000442\n",
      "13                   Education_MBA    0.000333\n",
      "11               Education_Diploma    0.000326\n",
      "9                    Education_BCA    0.000316\n",
      "\n",
      "Feature importance plot saved as 'feature_importances.png'.\n"
     ]
    }
   ],
   "source": [
    "if'best_model' in locals() and best_model is not None and 'preprocessor' in locals():\n",
    "    try:\n",
    "        importances = best_model.feature_importances_\n",
    "        num_features = numerical_features\n",
    "        ohe_trasformer = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_features_ohe = ohe_trasformer.get_feature_names_out(categorical_features)\n",
    "        all_processed_features = num_features + list(cat_features_ohe)\n",
    "\n",
    "        if len(importances) != len(all_processed_features):\n",
    "            print(f\"error: missmatch in feature count!.\")\n",
    "            print(f\"impertance found: {len(importances)}\")\n",
    "            print(f\"features found: {len(all_processed_features)}\")\n",
    "        else:\n",
    "            df_importances = pd.DataFrame({\n",
    "                'Feature': all_processed_features,\n",
    "                'Importance': importances\n",
    "            })\n",
    "\n",
    "            df_importances = df_importances.sort_values(by='Importance', ascending=False)\n",
    "            print(\"Model Feature Importances(Highest to lowest):\")\n",
    "            print(df_importances)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=df_importances.head(15), palette='viridis')\n",
    "\n",
    "            plt.title('Top 15 Feature Importances')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plot_filename = 'feature_importances.png'\n",
    "            plt.savefig(plot_filename)\n",
    "            print(f\"\\nFeature importance plot saved as '{plot_filename}'.\")\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compute feature importances: {e}\")\n",
    "else:\n",
    "    print(\"Model, or preprocessor not found in memory. Cannot compute feature importances.\")\n",
    "    print(\"Please ensure that the model training block has been executed successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ea96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Performance Metrics :\n",
      "\n",
      "MODEL PERFORMANCE (on Test Set)\n",
      "Accuracy: 0.9560\n",
      "Log Loss: 0.1828\n",
      "Precision (for 'Shortlisted'): 0.9536\n",
      "Recall (for 'Shortlisted'):    0.9592\n",
      "F1 Score (for 'Shortlisted'):  0.9564\n",
      "AUC:             0.9611\n",
      "ROC curve plot saved as 'roc_curve.png'\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Rejected       0.96      0.95      0.96       994\n",
      " Shortlisted       0.95      0.96      0.96      1006\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.96      0.96      0.96      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "Confusion matrix plot saved as 'confusion_matrix.png'\n"
     ]
    }
   ],
   "source": [
    "# Performance Metrics\n",
    "\n",
    "if best_model is not None and X_test_processed is not None and y_test is not None:\n",
    "    print(\"Generating Performance Metrics :\")\n",
    "    \n",
    "    # Calculate predictions and probabilities first :\n",
    "    y_pred = best_model.predict(X_test_processed)\n",
    "    y_proba_all = best_model.predict_proba(X_test_processed)\n",
    "    \n",
    "    # Calculate metrics that don't depend on a positive class :\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    loss = log_loss(y_test, y_proba_all) # Log loss\n",
    "\n",
    "    print(f\"\\nMODEL PERFORMANCE (on Test Set)\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Log Loss: {loss:.4f}\")\n",
    "\n",
    "    if positive_label_encoded in best_model.classes_:\n",
    "        # Positive class (e.g., 'Shortlisted') exists :\n",
    "        positive_class_index = np.where(best_model.classes_ == positive_label_encoded)[0][0]\n",
    "        y_proba_positive = y_proba_all[:, positive_class_index]\n",
    "        \n",
    "        # Get the name of the positive class for printing\n",
    "        positive_class_name = le_target.classes_[positive_label_encoded]\n",
    "        \n",
    "        # Calculate metrics for the positive class\n",
    "        precision = precision_score(y_test, y_pred, pos_label=positive_label_encoded, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=positive_label_encoded, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=positive_label_encoded)\n",
    "        auc = roc_auc_score(y_test, y_proba_positive)\n",
    "\n",
    "        print(f\"Precision (for '{positive_class_name}'): {precision:.4f}\")\n",
    "        print(f\"Recall (for '{positive_class_name}'):    {recall:.4f}\")\n",
    "        print(f\"F1 Score (for '{positive_class_name}'):  {f1:.4f}\")\n",
    "        print(f\"AUC:             {auc:.4f}\")\n",
    "\n",
    "        # Generate ROC Curve Plot :\n",
    "        try:\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_proba_positive, pos_label=positive_label_encoded)\n",
    "            \n",
    "            plt.figure(figsize=(7, 6))\n",
    "            plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.savefig(\"roc_curve.png\")\n",
    "            print(\"ROC curve plot saved as 'roc_curve.png'\")\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate ROC curve: {e}\")\n",
    "\n",
    "    else:\n",
    "        # Fallback for multi-class or if positive label is missing :\n",
    "        print(f\"Warning: Positive label {positive_label_encoded} not found. Calculating weighted metrics.\")\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        print(f\"Weighted Precision: {precision:.4f}\")\n",
    "        print(f\"Weighted Recall:    {recall:.4f}\")\n",
    "        print(f\"Weighted F1 Score:  {f1:.4f}\")\n",
    "        print(\"AUC/ROC curve is skipped (requires a defined positive class).\")\n",
    "\n",
    "    # Classification Report :\n",
    "    target_names = le_target.inverse_transform(np.unique(y_test))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "    # Confusion Matrix :\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=le_target.transform(target_names))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_names,\n",
    "                yticklabels=target_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    print(\"Confusion matrix plot saved as 'confusion_matrix.png'\")\n",
    "    plt.close()\n",
    "\n",
    "else:\n",
    "    print(\"Model, X_test_processed, or y_test not found.\")\n",
    "    print(\"Please run Above Block (Model Training) first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2439d067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLP model...\n",
      "NLP model loaded.\n",
      "All NLP and Prediction functions are defined (v12 - Now extracts all info).\n"
     ]
    }
   ],
   "source": [
    "# NLP & Prediction Functions\n",
    "\n",
    "print(\"Loading NLP model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading 'en_core_web_sm'...\")\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"NLP model loaded.\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                page_text = page.get_text()\n",
    "                if page_text.strip():\n",
    "                    text += page_text + \"\\n\"\n",
    "                else:\n",
    "                    # OCR for scanned resumes\n",
    "                    pix = page.get_pixmap()\n",
    "                    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                    text += pytesseract.image_to_string(img) + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "    return text\n",
    "\n",
    "def safe_int(val, default=0):\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            val = val.replace(\",\", \"\").strip()\n",
    "        return int(float(val))\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def safe_float(val, default=0.0):\n",
    "    try:\n",
    "        if isinstance(val, str):\n",
    "            val = val.replace(\",\", \"\").strip()\n",
    "        return float(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "def find_experience(text, keywords):\n",
    "    best_match = 0\n",
    "    text_norm = text.lower().replace('\\n', ' ')\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            for match in re.finditer(keyword, text_norm):\n",
    "                window_start = max(0, match.start() - 40)\n",
    "                search_window = text_norm[window_start : match.start()]\n",
    "                exp_match = re.search(r\"(\\d+)\\+?\\s*(?:year|yrs)\", search_window)\n",
    "                if exp_match:\n",
    "                    years = safe_int(exp_match.group(1))\n",
    "                    if years > best_match:\n",
    "                        best_match = years\n",
    "        except Exception as e:\n",
    "            print(f\"Error in find_experience with keyword '{keyword}': {e}\")\n",
    "    return best_match\n",
    "\n",
    "def parse_salary(salary_str):\n",
    "    salary_str = salary_str.lower().strip().replace(\",\", \"\")\n",
    "    number_match = re.search(r\"([\\d\\.]+)\", salary_str)\n",
    "    if not number_match: return 0\n",
    "    number = safe_float(number_match.group(1), 0)\n",
    "    if \"lpa\" in salary_str or \"lakh\" in salary_str:\n",
    "        return int(number * 100000)\n",
    "    elif number > 20000:\n",
    "        return int(number)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def find_salary(text, keywords):\n",
    "    text_norm = text.lower().replace('\\n', ' ')\n",
    "    salary_regex = r\"([\\d\\.]+\\s*(?:lpa|lakh))|(\\b[\\d,]+\\d{3}\\b)\"\n",
    "    for match in re.finditer(salary_regex, text_norm, re.IGNORECASE):\n",
    "        window_start = max(0, match.start() - 40)\n",
    "        search_window = text_norm[window_start:match.start()]\n",
    "        for keyword in keywords:\n",
    "            if keyword in search_window:\n",
    "                return parse_salary(match.group(0))\n",
    "    return 0\n",
    "\n",
    "# MAIN NLP FUNCTION :\n",
    "\n",
    "def extract_features_nlp(text, candidate_name=\"Unknown\"):\n",
    "    global feature_columns\n",
    "    if not feature_columns:\n",
    "        print(\"Error: feature_columns is not set. Please run Block 2 first.\")\n",
    "        return {}\n",
    "\n",
    "    # Initialize all expected features to 0\n",
    "    features = {key: 0 for key in feature_columns}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    text_with_spaces = text_lower.replace('\\n', ' ')\n",
    "\n",
    "    # Extract Personal Info (for the report) :\n",
    "    info_dict = {\n",
    "        \"Name\": candidate_name,\n",
    "        \"Email\": \"N/A\",\n",
    "        \"Phone No.\": \"N/A\",\n",
    "        \"Age\": \"N/A\",\n",
    "        \"Gender\": \"N/A\"\n",
    "    }\n",
    "\n",
    "    # Extract Email\n",
    "    email_match = re.search(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\", text_lower)\n",
    "    if email_match:\n",
    "        info_dict[\"Email\"] = email_match.group(0)\n",
    "\n",
    "    # Extract Phone\n",
    "    phone_match = re.search(r\"(\\+91[\\s-]?)?(\\b[\\d\\s-]{10}\\b)\", text)\n",
    "    if phone_match:\n",
    "        info_dict[\"Phone No.\"] = phone_match.group(2).replace(\" \", \"\").replace(\"-\", \"\")\n",
    "\n",
    "    # Extract Age\n",
    "    age_match = re.search(r\"age[:\\s]*(\\d{2})\", text_lower)\n",
    "    if age_match:\n",
    "        info_dict[\"Age\"] = safe_int(age_match.group(1))\n",
    "\n",
    "    # Extract Gender\n",
    "    gender_match = re.search(r\"gender[:\\s]*(male|female)\", text_lower, re.IGNORECASE)\n",
    "    if gender_match:\n",
    "        info_dict[\"Gender\"] = gender_match.group(1).capitalize()\n",
    "    elif \"\\bfemale\\b\" in text_lower:\n",
    "        info_dict[\"Gender\"] = \"Female\"\n",
    "    elif \"\\bmale\\b\" in text_lower:\n",
    "        info_dict[\"Gender\"] = \"Male\"\n",
    "\n",
    "    # Extract Training Features (for the model) :\n",
    "    \n",
    "    # Programming_Skills\n",
    "    prog_skills_list = ['python', 'java', 'c++', 'sql', 'javascript', 'react', 'node.js', 'pandas', 'scikit-learn', 'tensorflow']\n",
    "    found_skills = 0\n",
    "    for skill in prog_skills_list:\n",
    "        if skill in text_lower:\n",
    "            found_skills += 1\n",
    "    features['Programming_Skills'] = found_skills\n",
    "\n",
    "    # Coding_Skills_Years\n",
    "    features['Coding_Skills_Years'] = find_experience(text_lower, [\"coding\", \"programming\", \"developer\", \"software engineer\"])\n",
    "\n",
    "    # Job_Experience_NumCompanies\n",
    "    company_match = re.search(r\"(?:worked at|companies like|previous employers)\\s*([A-Z][\\w\\s,]+(?:(?:,| and)\\s*[A-Z][\\w\\s,]+)*)\", text)\n",
    "    if company_match:\n",
    "        companies_str = company_match.group(1).replace(\" and \", \", \")\n",
    "        features['Job_Experience_NumCompanies'] = len([c.strip() for c in companies_str.split(\",\") if c.strip() and c.strip() != \" \"])\n",
    "\n",
    "    # Salaries\n",
    "    features['Previous_Salary'] = find_salary(text_with_spaces, [\"previous\", \"current compensation\", \"current salary\"])\n",
    "    features['Desired_Salary'] = find_salary(text_with_spaces, [\"expected\", \"desired\", \"seeking\", \"target salary\"])\n",
    "\n",
    "    # Projects\n",
    "    projects_match = re.search(r\"(\\d+)\\s*(?:project|projects)\", text_lower)\n",
    "    if projects_match: \n",
    "        features['Projects'] = safe_int(projects_match.group(1))\n",
    "\n",
    "    # Internships\n",
    "    internship_match = re.search(r\"(\\d+)\\s*(?:internship|internships)\", text_lower)\n",
    "    if internship_match: \n",
    "        features['Internships'] = safe_int(internship_match.group(1))\n",
    "\n",
    "    # Education\n",
    "    stream_match = re.search(r\"\\b(b\\.?tech|m\\.?tech|b\\.?sc|m\\.?sc|b\\.?ca|m\\.?ca|b\\.?e\\.?|m\\.?b\\.?a|diploma|computer science)\\b\", text_lower)\n",
    "    features['Education'] = 'Diploma' # Default if nothing found\n",
    "    if stream_match:\n",
    "        s = stream_match.group(0).lower()\n",
    "        if \"b.tech\" in s or \"btech\" in s or \"b.e\" in s: features[\"Education\"] = \"B.E.\"\n",
    "        elif \"m.tech\" in s or \"mtech\" in s: features[\"Education\"] = \"M.Tech\"\n",
    "        elif \"b.sc\" in s or \"bsc\" in s or \"computer science\" in s: features[\"Education\"] = \"BSc Computer Science\"\n",
    "        elif \"bca\" in s: features[\"Education\"] = \"BCA\"\n",
    "        elif \"mba\" in s: features[\"Education\"] = \"MBA\"\n",
    "        elif \"mca\" in s: features[\"Education\"] = \"MCA\"\n",
    "        elif \"diploma\" in s: features[\"Education\"] = \"Diploma\"\n",
    "\n",
    "    # Gap_Years\n",
    "    gap_match = re.search(r\"gap of (\\d+)\\s*year\", text_lower)\n",
    "    if gap_match: \n",
    "        features['Gap_Years'] = safe_int(gap_match.group(1))\n",
    "\n",
    "    # Combine Dictionaries :\n",
    "    final_features_dict = {}\n",
    "    final_features_dict.update(info_dict) # Add personal info first\n",
    "    \n",
    "    # Add all the training features\n",
    "    for key in feature_columns:\n",
    "        final_features_dict[key] = features.get(key, 0)\n",
    "        \n",
    "    return final_features_dict\n",
    "\n",
    "def process_and_predict_resume(pdf_path):\n",
    "    global best_model, preprocessor, le_target, feature_columns\n",
    "    \n",
    "    if not best_model or not preprocessor:\n",
    "        raise Exception(\"Model or preprocessor is not trained. Please run Block 2.\")\n",
    "\n",
    "    name = os.path.basename(pdf_path).replace(\".pdf\", \"\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text.strip():\n",
    "        print(f\"Warning: No text extracted from {name}.\")\n",
    "        return {\"name\": name, \"Prediction\": \"Error - Empty PDF\"}\n",
    "\n",
    "    features_dict = extract_features_nlp(text, candidate_name=name)\n",
    "    \n",
    "    X_new = pd.DataFrame([features_dict])\n",
    "    \n",
    "    X_new_for_processing = X_new[feature_columns]\n",
    "\n",
    "    X_new_processed = preprocessor.transform(X_new_for_processing)\n",
    "    \n",
    "    prediction_encoded = best_model.predict(X_new_processed)[0]\n",
    "    prediction_label = le_target.inverse_transform([prediction_encoded])[0]\n",
    "    \n",
    "    result_icon = \" Shortlisted\" if 'shortlist' in prediction_label.lower() else \" Rejected\"\n",
    "    \n",
    "    # Add all extracted features to the final dictionary\n",
    "    features_dict[\"Prediction\"] = result_icon\n",
    "    return features_dict\n",
    "\n",
    "print(\"All NLP and Prediction functions are defined (v12 - Now extracts all info).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching GUI :\n"
     ]
    }
   ],
   "source": [
    "# The GUI Application\n",
    "\n",
    "print(\"Launching GUI :\")\n",
    "\n",
    "# PATHS (from your code) :\n",
    "RESUME_FOLDER = r\"D:\\AICreation\\Resumes\"\n",
    "OUTPUT_FILE = r\"D:\\AICreation\\Resume_Results.xlsx\"\n",
    "os.makedirs(RESUME_FOLDER, exist_ok=True)\n",
    "\n",
    "# This list will hold the *file names* of dropped files\n",
    "staged_files = [] \n",
    "\n",
    "# Thread-Safe Prediction Logic :\n",
    "\n",
    "def process_resumes_background(root, analyze_btn, progress_label, progress_bar):\n",
    "    # Get resumes from the staged_files list\n",
    "    total = len(staged_files)\n",
    "    if total == 0:\n",
    "        root.after(0, lambda: messagebox.showwarning(\"No Resumes\", \"Please drag and drop PDF resumes first!\"))\n",
    "        root.after(0, on_analysis_complete, analyze_btn, progress_label, progress_bar, 0, False)\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, pdf_name in enumerate(staged_files, start=1):\n",
    "        pdf_path = os.path.join(RESUME_FOLDER, pdf_name)\n",
    "        try:\n",
    "            # This will now return the new features\n",
    "            result_dict = process_and_predict_resume(pdf_path)\n",
    "            \n",
    "            progress_percent = int((i / total) * 100)\n",
    "            # Schedule progress bar update\n",
    "            root.after(0, progress_bar.config, {\"value\": progress_percent})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdf_name}: {e}\")\n",
    "            result_dict = {\"name\": pdf_name.replace(\".pdf\", \"\"), \"Prediction\": f\"Error: {e}\"}\n",
    "        \n",
    "        results.append(result_dict)\n",
    "\n",
    "    # Analysis Done, now create Excel :\n",
    "    try:\n",
    "        df_result = pd.DataFrame(results)\n",
    "        if \"Prediction\" in df_result.columns:\n",
    "            # Move Prediction column to the end in Excel\n",
    "            cols = [c for c in df_result.columns if c != \"Prediction\"] + [\"Prediction\"]\n",
    "            df_result = df_result[cols]\n",
    "            \n",
    "        df_result.to_excel(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        root.after(0, on_analysis_complete, analyze_btn, progress_label, progress_bar, total, True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        root.after(0, lambda: messagebox.showerror(\"Error\", f\"Failed to save Excel file: {e}\"))\n",
    "        root.after(0, on_analysis_complete, analyze_btn, progress_label, progress_bar, 0, False)\n",
    "\n",
    "\n",
    "def on_analysis_complete(analyze_btn, progress_label, progress_bar, total_files, success):\n",
    "    \n",
    "    # This function runs on the MAIN GUI THREAD.\n",
    "    \n",
    "    global staged_files\n",
    "    \n",
    "    if success:\n",
    "        progress_label.config(text=\" Analysis complete! Opening Excel file...\")\n",
    "        messagebox.showinfo(\"Complete\", f\"Analysis complete for {total_files} resumes.\")\n",
    "        \n",
    "        clear_all_resumes(progress_label)\n",
    "        \n",
    "        # Auto-open the Excel file\n",
    "        try:\n",
    "            os.startfile(OUTPUT_FILE)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                import subprocess\n",
    "                if sys.platform == \"darwin\":      # macOS\n",
    "                    subprocess.call((\"open\", OUTPUT_FILE))\n",
    "                elif sys.platform.startswith(\"linux\"):  # Linux\n",
    "                    subprocess.call((\"xdg-open\", OUTPUT_FILE))\n",
    "            except:\n",
    "                 messagebox.showinfo(\"File Saved\", f\"Results saved at:\\n{OUTPUT_FILE}\\n\\nUnable to auto-open: {e}\")\n",
    "    \n",
    "    # Reset the GUI\n",
    "    analyze_btn.config(text=\" Start Analysis\", state=\"normal\")\n",
    "    if not success:\n",
    "        progress_label.config(text=\"Analysis failed or was canceled.\")\n",
    "    progress_bar.config(value=0)\n",
    "\n",
    "\n",
    "def start_analysis_thread(root, analyze_btn, progress_label, progress_bar, drop_area):\n",
    "    \n",
    "    #This is the main button-click function.\n",
    "    \n",
    "    analyze_btn.config(text=\"Analyzing...\", state=\"disabled\")\n",
    "    progress_label.config(text=\"Starting analysis...\")\n",
    "    \n",
    "    threading.Thread(\n",
    "        target=process_resumes_background,\n",
    "        args=(root, analyze_btn, progress_label, progress_bar),\n",
    "        daemon=True\n",
    "    ).start()\n",
    "\n",
    "\n",
    "# File Drop Handler :\n",
    "def on_drop(event, drop_area):\n",
    "    global staged_files\n",
    "    files = drop_area.tk.splitlist(event.data)\n",
    "    count = 0\n",
    "    for file_path in files:\n",
    "        if file_path.lower().endswith(\".pdf\"):\n",
    "            file_name = os.path.basename(file_path)\n",
    "            dest_path = os.path.join(RESUME_FOLDER, file_name)\n",
    "            \n",
    "            if file_name not in staged_files:\n",
    "                shutil.copy(file_path, dest_path)\n",
    "                staged_files.append(file_name) # Add to our list\n",
    "                count += 1\n",
    "            else:\n",
    "                print(f\"Skipped: {file_name} is already in the queue.\")\n",
    "    \n",
    "    drop_area.config(text=f\"{len(staged_files)} file(s) in queue. Drop more or Analyze.\")\n",
    "    if count > 0:\n",
    "        messagebox.showinfo(\"Files Added\", f\"Added {count} new PDF(s) to the queue.\")\n",
    "\n",
    "# NEW Delete Functions :\n",
    "def delete_previous_file(drop_area):\n",
    "    #Deletes the last file added to the queue.\n",
    "    global staged_files\n",
    "    if not staged_files:\n",
    "        messagebox.showwarning(\"Empty\", \"The resume queue is already empty.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        file_name = staged_files.pop() # Remove last item\n",
    "        file_path = os.path.join(RESUME_FOLDER, file_name)\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            \n",
    "        print(f\"Deleted: {file_name}\")\n",
    "        messagebox.showinfo(\"File Removed\", f\"Removed last file: {file_name}\")\n",
    "        drop_area.config(text=f\"{len(staged_files)} file(s) in queue. Drop more or Analyze.\")\n",
    "        if not staged_files:\n",
    "             drop_area.config(text=\"Drop PDFs Here\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Could not delete {file_name}: {e}\")\n",
    "\n",
    "def clear_all_resumes(drop_area_or_label):\n",
    "    # Deletes all resumes from the folder and clears the list.\n",
    "    global staged_files\n",
    "    if not staged_files:\n",
    "        messagebox.showinfo(\"Empty\", \"The resume queue is already empty.\")\n",
    "        return\n",
    "    \n",
    "    if messagebox.askyesno(\"Confirm Clear All\", f\"Are you sure you want to clear all {len(staged_files)} resumes from the queue?\"):\n",
    "        try:\n",
    "            for file_name in staged_files:\n",
    "                file_path = os.path.join(RESUME_FOLDER, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    os.remove(file_path)\n",
    "            \n",
    "            staged_files.clear()\n",
    "            print(\"Cleared all resumes from queue and folder.\")\n",
    "            if isinstance(drop_area_or_label, tk.Label):\n",
    "                drop_area_or_label.config(text=\"Drop PDFs Here\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred while clearing resumes: {e}\")\n",
    "\n",
    "# Tkinter GUI :\n",
    "if 'best_model' in locals() and best_model is not None:\n",
    "    root = TkinterDnD.Tk()\n",
    "    root.title(\"AI Resume Analyzer\")\n",
    "    root.geometry(\"500x450\")\n",
    "    root.config(bg=\"#f2f2f2\")\n",
    "\n",
    "    Label = tk.Label\n",
    "    style = ttk.Style()\n",
    "    style.configure(\"TButton\", font=(\"Arial\", 12), padding=10)\n",
    "    style.configure(\"Small.TButton\", font=(\"Arial\", 9), padding=4)\n",
    "\n",
    "    Label(root, text=\"Drag & Drop PDF Resumes Below\", font=(\"Arial\", 14, \"bold\"), bg=\"#f2f2f2\").pack(pady=10)\n",
    "\n",
    "    drop_area = tk.Label(root, text=\"Drop PDFs Here\", relief=\"ridge\", borderwidth=3,\n",
    "                         width=50, height=8, bg=\"white\", fg=\"gray\", font=(\"Arial\", 11))\n",
    "    drop_area.pack(pady=10, padx=40, fill=\"x\")\n",
    "\n",
    "    drop_area.drop_target_register(DND_FILES)\n",
    "    drop_area.dnd_bind('<<Drop>>', lambda e: on_drop(e, drop_area))\n",
    "    \n",
    "    # NEW DELETE BUTTONS FRAME :\n",
    "    delete_frame = ttk.Frame(root, style=\"TLabel\")\n",
    "    delete_frame.pack(fill=\"x\", padx=40, pady=(0, 5))\n",
    "\n",
    "    btn_delete_selected = ttk.Button(delete_frame, text=\"Delete Previous File\", style=\"Small.TButton\",\n",
    "                                     command=lambda: delete_previous_file(drop_area))\n",
    "    btn_delete_selected.pack(side=\"left\", expand=True, fill=\"x\", padx=2)\n",
    "    \n",
    "    btn_delete_all = ttk.Button(delete_frame, text=\"Delete All Files\", style=\"Small.TButton\",\n",
    "                                     command=lambda: clear_all_resumes(drop_area))\n",
    "    btn_delete_all.pack(side=\"left\", expand=True, fill=\"x\", padx=2)\n",
    "    \n",
    "    # PROGRESS BAR AND LABEL :\n",
    "    progress_var = tk.IntVar()\n",
    "    progress_bar = ttk.Progressbar(root, orient=\"horizontal\", length=350, mode=\"determinate\", variable=progress_var)\n",
    "    progress_bar.pack(pady=15)\n",
    "\n",
    "    progress_label = Label(root, text=\"Ready to analyze.\", font=(\"Arial\", 10), bg=\"#f2f2f2\")\n",
    "    progress_label.pack()\n",
    "\n",
    "    analyze_btn = ttk.Button(root, text=\" Start Analysis\",\n",
    "                             command=lambda: start_analysis_thread(root, analyze_btn, progress_label, progress_bar, drop_area)\n",
    "    )\n",
    "    analyze_btn.pack(pady=20)\n",
    "    \n",
    "    # POPULATE STAGED LIST ON START :\n",
    "    try:\n",
    "        existing_files = [f for f in os.listdir(RESUME_FOLDER) if f.lower().endswith(\".pdf\")]\n",
    "        if existing_files:\n",
    "            staged_files = existing_files\n",
    "            drop_area.config(text=f\"{len(staged_files)} file(s) in queue.\")\n",
    "            print(f\"Loaded {len(existing_files)} existing resumes from {RESUME_FOLDER}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing files: {e}\")\n",
    "\n",
    "\n",
    "    # THIS IS THE BLOCKING CALL :\n",
    "    root.mainloop()\n",
    "    \n",
    "else:\n",
    "    print(\"GUI launch skipped because the model was not trained.\")\n",
    "    print(\"Please run full program successfully and then run this again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
